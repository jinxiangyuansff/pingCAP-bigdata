# pingCAP-bigdata
这是一个大数据作业（10gurl文件，用1G内存找出最高频的前1000个单词）


思路：

这道题主要是分而治之的思想：

1.先把10G文件读取 **依次进行 hash（x）%50**,对url进行hash的原因是把不同长度的url地址固定为等长，方便进行分类。
而%50是得到相同hash返回值的url被放到一个文件上，因此可以把10G文件大概分成几乎相等的50份。假如有的文件依然超过1G，则继续进行hash。

2.得到了50份文件，每份文件大概是200M.这个时候1G内存足够读取整个文件。**建立一个容量为100的小顶堆**，这个时候可以使用一个**hashmap依次统计url出现的频率**，其中key为url，value为出现次数。然后再用**容量为100小顶堆**统计出每个小文件的top100.然后放入外存，这时候多出了50个前100单词文件。

3.统计完50份左右的文件后，**建立一个容量为100的小顶堆**，**然后建立一个hashmap**，统计50个文件的前100，如果有比小顶堆堆顶元素更多的词，则堆顶元素移除，然后加入该新的url，重新调整堆顶元素。这样就能得到总的top100url。


具体代码实现：

1.先在ChannelFileReader 把大文件按 hash(url)%50 得到50份小文件  

2.然后每一份小文件用hashMap统计url出现次数，并各自创建一个size为100的小顶堆。堆节点为hashmap的entry节点。  

3.50个小顶堆两两合并，最终得到一个总的size为100的小顶堆








